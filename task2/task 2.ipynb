{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = pd.read_csv(\"C:/Users/dell/Downloads/kelper_data.csv\")\n",
    "dfc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['koi_disposition'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['koi_pdisposition'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2418 candidates\n",
    "\n",
    "(dfc['koi_disposition'] == \"CANDIDATE\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.head(10)    # first 20 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the columns\n",
    "\n",
    "dfc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the non-numeric columns\n",
    "\n",
    "df_numeric = dfc.copy()\n",
    "\n",
    "koi_disposition_labels = {\n",
    "    \"koi_disposition\": {\n",
    "        \"CONFIRMED\": 1,\n",
    "        \"FALSE POSITIVE\": 0,\n",
    "        \"CANDIDATE\": 2,\n",
    "        \"NOT DISPOSITIONED\": 3\n",
    "    },\n",
    "    \"koi_pdisposition\": {\n",
    "        \"CONFIRMED\": 1,\n",
    "        \"FALSE POSITIVE\": 0,\n",
    "        \"CANDIDATE\": 2,\n",
    "        \"NOT DISPOSITIONED\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "df_numeric.replace(koi_disposition_labels, inplace=True)\n",
    "df_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is train data\n",
    "\n",
    "# first we remove all string type columns from the dataframe\n",
    "\n",
    "df_numeric = df_numeric.select_dtypes(exclude=['object']).copy()\n",
    "df_test = df_numeric.copy()    # test data\n",
    "\n",
    "# second, we manually remove some columns which are not needed as mentioned above. \n",
    "# additionally, 'koi_teq_err1' and 'koi_teq_err2' have all null values so they too need to be removed\n",
    "\n",
    "rem_cols = ['kepid', 'koi_pdisposition', 'koi_score', 'koi_time0bk', 'koi_time0bk_err1', 'koi_time0bk_err2', 'koi_teq_err1', 'koi_teq_err2']\n",
    "df_numeric.drop(rem_cols, axis=1, inplace=True)\n",
    "\n",
    "# this is test data\n",
    "rem_cols_test = [col for col in rem_cols if col not in ['koi_pdisposition', 'koi_score']]\n",
    "df_test.drop(rem_cols_test, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_numeric.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df_numeric[df_numeric.isnull().sum(axis=1) == 0]\n",
    "df_numeric.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df_numeric[df_numeric.koi_fpflag_nt == df_numeric.koi_fpflag_nt.max()].index\n",
    "df_numeric.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[df_test.isnull().sum(axis=1) == 0]\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[df_test.koi_disposition == 2]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('koi_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.to_csv('koi_numeric.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric1 = df_numeric.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "sns.heatmap(df_numeric1.corr(), annot=True, cmap=\"RdYlGn\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# need to exclude the `koi_disposition` column from being standardized\n",
    "\n",
    "\n",
    "df_numeric1.iloc[:, 5:] = std_scaler.fit_transform(df_numeric1.iloc[:, 5:])\n",
    "\n",
    "\n",
    "# df_numeric.iloc[:, 0].to_numpy().reshape(-1, 1).shape\n",
    "# df_standardized_w_labels = np.c_[df_standardized, df_numeric.iloc[:, 0].to_numpy().reshape(-1, 1)]\n",
    "# df_standardized_w_labels[:3]\n",
    "\n",
    "df_numeric1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeplerDataset(Dataset):\n",
    "    def __init__(self, test=False):\n",
    "        self.dataframe_orig = pd.read_csv(\"C:/Users/dell/Downloads/kelper_data.csv\")\n",
    "\n",
    "        if (test == False):\n",
    "            self.data = df_numeric1[( df_numeric1.koi_disposition == 1 ) | ( df_numeric1.koi_disposition == 0 )].values\n",
    "        else:\n",
    "            self.data = df_numeric1[~(( df_numeric1.koi_disposition == 1 ) | ( df_numeric1.koi_disposition == 0 ))].values\n",
    "            \n",
    "        self.X_data = torch.FloatTensor(self.data[:, 1:])\n",
    "        self.y_data = torch.FloatTensor(self.data[:, 0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    \n",
    "    def get_col_len(self):\n",
    "        return self.X_data.shape[1]\n",
    "    \n",
    "kepler_df = KeplerDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature, target = kepler_df[1]\n",
    "target, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kepler_df.get_col_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and validation set\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "split_ratio = .7 # 70 / 30 split\n",
    "\n",
    "train_size = int(len(kepler_df) * split_ratio)\n",
    "val_size = len(kepler_df) - train_size\n",
    "train_ds, val_ds = random_split(kepler_df, [train_size, val_size])\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, target in train_loader:\n",
    "    print(features.size(), target.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KOIClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        super(KOIClassifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 32)    \n",
    "        self.linear2 = nn.Linear(32, 32)\n",
    "        self.linear3 = nn.Linear(32, 16)\n",
    "        self.linear4 = nn.Linear(16, 8)\n",
    "        self.linear5 = nn.Linear(8, out_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.linear1(xb)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.linear2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.linear3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.linear4(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.linear5(out)\n",
    "        out = torch.sigmoid(out)\n",
    "\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        pred = self.forward(x)\n",
    "        return pred\n",
    "    \n",
    "        \n",
    "    def print_params(self):\n",
    "        for params in self.parameters():\n",
    "            print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = kepler_df.get_col_len()\n",
    "out_dim = 1\n",
    "model = KOIClassifier(input_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training phase\n",
    "criterion = nn.BCELoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "n_epochs = 1000\n",
    "\n",
    "def train_model():\n",
    "    for X, y in train_loader:\n",
    "        for epoch in range(n_epochs):\n",
    "            optim.zero_grad()\n",
    "            y_pred = model.forward(X).flatten()\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the predictions\n",
    "for X, y in train_loader:\n",
    "    y_pred = model.forward(X)\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_pred = torch.tensor(y_pred, dtype=torch.int32)\n",
    "    print(y_pred)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def pred_confusion_matrix(model, loader):\n",
    "    with torch.no_grad():\n",
    "        all_preds = torch.tensor([])\n",
    "        all_true = torch.tensor([])\n",
    "        for X, y in loader:\n",
    "            y_pred = model(X)\n",
    "            y_pred = torch.tensor(y_pred > 0.5, dtype=torch.float32).flatten()\n",
    "            all_preds = torch.cat([all_preds, y_pred])\n",
    "\n",
    "            all_true = torch.cat([all_true, y])\n",
    "            \n",
    "    \n",
    "    return confusion_matrix(all_true.numpy(), all_preds.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "cf_mat_train = pred_confusion_matrix(model, train_loader)\n",
    "cf_mat_val = pred_confusion_matrix(model, val_loader)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n",
    "\n",
    "ax1, ax2 = axes\n",
    "sns.heatmap(cf_mat_train, fmt='g', annot=True, ax=ax1)\n",
    "ax1.set_title('Training Data')\n",
    "\n",
    "sns.heatmap(cf_mat_val, fmt='g', annot=True, ax=ax2)\n",
    "ax2.set_title('Validation Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optim.state_dict()\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where we return back to the point from where we branched, we take the numeric dataframe again and apply some feature selection\n",
    "df_new = pd.read_csv('koi_numeric.csv', index_col=0)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to remove high correlation columns by selecting the upper triangle of the correlation matrix\n",
    "# and dropping all columns which have corr value > threshold at any row\n",
    "\n",
    "def remove_high_corr(df, threshold):\n",
    "    corr_mat = df.corr()\n",
    "    trimask = corr_mat.abs().mask(~np.triu(np.ones(corr_mat.shape, dtype=bool), k=1))\n",
    "    blocklist = [col for col in trimask.columns if (trimask[col] > threshold).any()]\n",
    "    df.drop(columns=blocklist, axis=1,inplace=True)\n",
    "    return blocklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_high_corr(df_new, 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "sns.heatmap(df_new.corr(), cmap=\"Blues\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('koi_numeric_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying GPU accelaration, new Dataset and model architecture\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "    \n",
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "dataframe = pd.read_csv('koi_numeric_reduced.csv', index_col=0)\n",
    "\n",
    "train_data = dataframe.query('not koi_disposition == 2').values\n",
    "\n",
    "X = train_data[:, 1:]\n",
    "y = train_data[:, 0]\n",
    "\n",
    "val_size = .3\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=val_size, shuffle=True)\n",
    "\n",
    "train_X[:, 4:] = std_scaler.fit_transform(train_X[:, 4:])\n",
    "val_X[:, 4:] = std_scaler.fit_transform(val_X[:, 4:])\n",
    "\n",
    "\n",
    "# print(f'train_X = {train_X.shape}\\n\\nval_X = {val_X.shape}\\n')\n",
    "\n",
    "class KOIDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = torch.FloatTensor(X_data)\n",
    "        self.y_data = torch.FloatTensor(y_data)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "train_ds = KOIDataset(train_X, train_y)\n",
    "val_ds = KOIDataset(val_X, val_y)\n",
    "\n",
    "for feature, target in train_ds:\n",
    "    print(feature, target)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, target in train_loader:\n",
    "    print(target, features)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to measure prediction accuracy \n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    output_labels = torch.round(torch.sigmoid(outputs))    # manually have to activate sigmoid since the nn does not incorporate sigmoid at final layer\n",
    "    \n",
    "    return torch.tensor(torch.sum(output_labels == labels.unsqueeze(1)).item() / len(output_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "input_dim = train_X.shape[1]\n",
    "\n",
    "class KOIClassifierSeq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KOIClassifierSeq, self).__init__()\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "              ('fc1', nn.Linear(input_dim, 24)),\n",
    "              ('sigmoid1', nn.Sigmoid()),\n",
    "              ('batchnorm1', nn.BatchNorm1d(24)),\n",
    "              ('fc2', nn.Linear(24, 16)),\n",
    "              ('sigmoid2', nn.Sigmoid()),\n",
    "              ('batchnorm2', nn.BatchNorm1d(16)),\n",
    "              ('dropout', nn.Dropout(p=0.1)),\n",
    "              ('fc3', nn.Linear(16, 1))\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.model(xb)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        features, label = batch \n",
    "        out = self(features)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label.unsqueeze(1)) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        features, label = batch \n",
    "        out = self(features)                    \n",
    "        loss = F.binary_cross_entropy_with_logits(out, label.unsqueeze(1))   # Calculate loss\n",
    "        acc = accuracy(out, label)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = to_device(KOIClassifierSeq(), device)\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let us fit our model using Adam optimiser and a small learning rate `1e-5`\n",
    "num_epochs = 10\n",
    "lr = 1e-4\n",
    "history = fit(num_epochs, lr, model1, train_loader, val_loader, opt_func=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to calculate training accuracy\n",
    "\n",
    "def train_accuracy(model):\n",
    "    train_acc = []\n",
    "    for X, y in train_loader:\n",
    "        out = model(X)\n",
    "        train_acc.append(accuracy(out, y))\n",
    "\n",
    "    return torch.stack(train_acc).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, at the end of training which was relatively fast, We have 97.7% training accuracy and 97.2% validation accuracy.Let us calculate confusion matrix and visualize our predictions.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def pred_confusion_matrix(model, loader):\n",
    "    with torch.no_grad():\n",
    "        all_preds = to_device(torch.tensor([]), device)\n",
    "        all_true = to_device(torch.tensor([]), device)\n",
    "        for X, y in loader:\n",
    "            y_pred = model(X)\n",
    "            y_pred = torch.round(torch.sigmoid(y_pred))\n",
    "            all_preds = torch.cat([all_preds, y_pred])\n",
    "\n",
    "            all_true = torch.cat([all_true, y.unsqueeze(1)])\n",
    "            \n",
    "    \n",
    "    return confusion_matrix(all_true.cpu().numpy(), all_preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_mat_train = pred_confusion_matrix(model1, train_loader)\n",
    "cf_mat_val = pred_confusion_matrix(model1, val_loader)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n",
    "\n",
    "ax1, ax2 = axes\n",
    "sns.heatmap(cf_mat_train, fmt='g', annot=True, ax=ax1)\n",
    "ax1.set_title('Training Data')\n",
    "\n",
    "sns.heatmap(cf_mat_val, fmt='g', annot=True, ax=ax2)\n",
    "ax2.set_title('Validation Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs')\n",
    "    \n",
    "def plot_losses(history):\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = {\n",
    "    'state_dict': model1.state_dict()\n",
    "}\n",
    "\n",
    "torch.save(second_model, 'second_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('koi_test.csv', index_col=0)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    " 'koi_disposition',\n",
    " 'koi_pdisposition',\n",
    " 'koi_period_err2',\n",
    " 'koi_impact_err2',\n",
    " 'koi_duration_err2',\n",
    " 'koi_depth_err2',\n",
    " 'koi_prad_err2',\n",
    " 'koi_insol_err1',\n",
    " 'koi_insol_err2',\n",
    " 'koi_steff_err2',\n",
    " 'koi_srad_err2']\n",
    "\n",
    "test_df.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_df.iloc[:, 1:].values\n",
    "test_probs = test_df.iloc[:, 0].values\n",
    "\n",
    "test_X[:, 4:] = std_scaler.fit_transform(test_X[:, 4:])\n",
    "\n",
    "  \n",
    "\n",
    "KOI_test = KOIDataset(test_X, test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_loader = DataLoader(KOI_test, batch_size, num_workers=4, pin_memory=True)\n",
    "test_loader = DeviceDataLoader(test_loader, device)\n",
    "\n",
    "for X, y in test_loader:\n",
    "    print(X.size(), y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(model, X):\n",
    "    probs = torch.sigmoid(model(X))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=5, threshold=5000)\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        #print(X, y)\n",
    "        preds = torch.sigmoid(model1(X))\n",
    "        for pred, true in zip(preds, y.unsqueeze(1)):\n",
    "            print(f'model prediction: {pred.item()}\\tKOI prediction: {true.item()}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(outputs, label_prob):\n",
    "    output_labels = torch.round(torch.sigmoid(outputs))    \n",
    "    labels = torch.round(label_prob)\n",
    "    return torch.tensor(torch.sum(output_labels == labels.unsqueeze(1)).item() / len(output_labels))\n",
    "    \n",
    "    \n",
    "def test_accuracy(model):\n",
    "    test_acc = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            out = model(X)\n",
    "            test_acc.append(accuracy_test(out, y))\n",
    "\n",
    "    return torch.stack(test_acc).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), 'final_model_53_percent.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KOIClassifierSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KOIClassifierSimple, self).__init__()\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "              ('fc1', nn.Linear(input_dim, 24)),\n",
    "              ('sigmoid1', nn.Sigmoid()),\n",
    "              ('fc2', nn.Linear(24, 16)),\n",
    "              ('sigmoid2', nn.Sigmoid()),\n",
    "              ('fc3', nn.Linear(16, 1))\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.model(xb)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        features, label = batch \n",
    "        out = self(features)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label.unsqueeze(1)) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        features, label = batch \n",
    "        out = self(features)                    \n",
    "        loss = F.binary_cross_entropy_with_logits(out, label.unsqueeze(1))   # Calculate loss\n",
    "        acc = accuracy(out, label)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = to_device(KOIClassifierSimple(), device)\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 1e-3\n",
    "history2 = fit(num_epochs, lr, model2, train_loader, val_loader, opt_func=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy(model2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
